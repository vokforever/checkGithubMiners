import asyncio
import json
import os
import logging
import sys
import locale
import ctypes
import traceback
import shutil
import re
from datetime import datetime, timezone, timedelta
from typing import Dict, Optional, List, Set, Tuple
from aiohttp import ClientSession, ClientError, ClientResponseError, ClientTimeout, TCPConnector
from aiogram import Bot, Dispatcher, types, F
from aiogram.filters import CommandStart, Command
from aiogram.types import Message, CallbackQuery
from aiogram.utils.keyboard import InlineKeyboardBuilder
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from bs4 import BeautifulSoup
from dotenv import load_dotenv
import time
import gc

# –ò–º–ø–æ—Ä—Ç —Å–æ–≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä–∞
try:
    from modern_telegram_formatter import formatter, convert_markdown_to_telegram
    MODERN_FORMATTER_AVAILABLE = True
except ImportError:
    MODERN_FORMATTER_AVAILABLE = False
    logging.warning("–°–æ–≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç—Ç–µ—Ä –Ω–µ –¥–æ—Å—Ç—É–ø–µ–Ω, –∏—Å–ø–æ–ª—å–∑—É—é—Ç—Å—è –±–∞–∑–æ–≤—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏")

# --- –ù–ê–°–¢–†–û–ô–ö–ê –ö–û–î–ò–†–û–í–ö–ò –î–õ–Ø WINDOWS ---
if sys.platform == "win32":
    sys.stdout.reconfigure(encoding='utf-8', errors='backslashreplace')
    sys.stderr.reconfigure(encoding='utf-8', errors='backslashreplace')
    try:
        kernel32 = ctypes.windll.kernel32
        kernel32.SetConsoleOutputCP(65001)
    except Exception as e:
        print(f"–ù–µ —É–¥–∞–ª–æ—Å—å —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –∫–æ–¥–∏—Ä–æ–≤–∫—É UTF-8: {e}")

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ï –ù–ê–°–¢–†–û–ô–ö–ò –î–õ–Ø –°–õ–ê–ë–û–ì–û VPS ---
BOT_TOKEN = os.getenv("BOT_TOKEN")
GITHUB_TOKEN = os.getenv("GITHUB_TOKEN", None)
CHANNEL_ID = os.getenv("CHANNEL_ID")
ADMIN_ID = int(os.getenv("ADMIN_ID", "0"))
DONATE_URL = "https://boosty.to/vokforever/donate"

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è —Å–ª–∞–±–æ–≥–æ VPS
MAX_RETRIES = 2  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 3 –¥–æ 2
RETRY_DELAY = 1  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 2 –¥–æ 1
HISTORY_DAYS = 14  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 30 –¥–æ 14 –¥–Ω–µ–π
PRIORITY_UPDATE_DAYS = 7

# –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∏–Ω—Ç–µ—Ä–≤–∞–ª—ã –ø—Ä–æ–≤–µ—Ä–∫–∏ (—Å —É—á–µ—Ç–æ–º —Å–∏—Å—Ç–µ–º—ã –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤)
MIN_CHECK_INTERVAL_MINUTES = 30  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 15 –¥–æ 30 –º–∏–Ω—É—Ç
MAX_CHECK_INTERVAL_MINUTES = 2880  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 1440 –¥–æ 2880 (48 —á–∞—Å–æ–≤)
DEFAULT_CHECK_INTERVAL_MINUTES = 720  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 360 –¥–æ 720 (12 —á–∞—Å–æ–≤)

# –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —É–ø—Ä–∞–≤–ª–µ–Ω–∏—è —Ä–µ—Å—É—Ä—Å–∞–º–∏
MAX_CONCURRENT_REQUESTS = 3  # –ú–∞–∫—Å–∏–º—É–º –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
REQUEST_TIMEOUT = 20  # –£–º–µ–Ω—å—à–µ–Ω–æ —Å 30 –¥–æ 20 —Å–µ–∫—É–Ω–¥
BATCH_SIZE = 5  # –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏
MEMORY_THRESHOLD_MB = 100  # –ü–æ—Ä–æ–≥ –ø–∞–º—è—Ç–∏ –¥–ª—è –æ—á–∏—Å—Ç–∫–∏

# --- –°–ü–ò–°–û–ö –†–ï–ü–û–ó–ò–¢–û–†–ò–ï–í ---
REPOS = [
    "andru-kun/wildrig-multi",
    "OneZeroMiner/onezerominer", 
    "trexminer/T-Rex",
    "xmrig/xmrig",
    "Lolliedieb/lolMiner-releases",
    "doktor83/SRBMiner-Multi",
    "nicehash/nicehashminer",
    "pooler/cpuminer",
    "rplant8/cpuminer-opt-rplant",
    "JayDDee/cpuminer-opt",
    "alephium/gpu-miner"
]

# --- –§–ê–ô–õ–´ –•–†–ê–ù–ï–ù–ò–Ø –î–ê–ù–ù–´–• ---
STATE_FILE = "last_releases.json"
FILTERS_FILE = "user_filters.json"
HISTORY_FILE = "releases_history.json"
USERS_FILE = "users.json"
PRIORITY_FILE = "repo_priority.json"
STATISTICS_FILE = "bot_statistics.json"
CACHE_FILE = "github_cache.json"

# –°–æ–∑–¥–∞–µ–º –ø–∞–ø–∫—É –¥–ª—è —Ä–µ–∑–µ—Ä–≤–Ω—ã—Ö –∫–æ–ø–∏–π
BACKUP_DIR = "backups"
if not os.path.exists(BACKUP_DIR):
    os.makedirs(BACKUP_DIR)

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ï –õ–û–ì–ò–†–û–í–ê–ù–ò–ï ---
def setup_logging():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Å–∏—Å—Ç–µ–º—ã –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è VPS"""
    log_dir = "logs"
    if not os.path.exists(log_dir):
        os.makedirs(log_dir)
    
    # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –ª–æ–≥–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
    log_file = os.path.join(log_dir, "bot.log")
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è
    formatter = logging.Formatter(
        '%(asctime)s - %(name)s - %(levelname)s - %(message)s',
        datefmt='%Y-%m-%d %H:%M:%S'
    )
    
    # –§–∞–π–ª–æ–≤—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫ —Å —Ä–æ—Ç–∞—Ü–∏–µ–π
    file_handler = logging.FileHandler(log_file, encoding='utf-8')
    file_handler.setFormatter(formatter)
    file_handler.setLevel(logging.INFO)  # –¢–æ–ª—å–∫–æ INFO –∏ –≤—ã—à–µ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ –º–µ—Å—Ç–∞
    
    # –ö–æ–Ω—Å–æ–ª—å–Ω—ã–π –æ–±—Ä–∞–±–æ—Ç—á–∏–∫
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(formatter)
    console_handler.setLevel(logging.WARNING)  # –¢–æ–ª—å–∫–æ WARNING –∏ –≤—ã—à–µ –≤ –∫–æ–Ω—Å–æ–ª—å
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ—Ä–Ω–µ–≤–æ–≥–æ –ª–æ–≥–≥–µ—Ä–∞
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    root_logger.addHandler(file_handler)
    root_logger.addHandler(console_handler)
    
    # –û—Ç–∫–ª—é—á–∞–µ–º –ª–æ–≥–∏ –æ—Ç —Å—Ç–æ—Ä–æ–Ω–Ω–∏—Ö –±–∏–±–ª–∏–æ—Ç–µ–∫ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
    logging.getLogger('aiohttp').setLevel(logging.WARNING)
    logging.getLogger('aiogram').setLevel(logging.WARNING)
    logging.getLogger('apscheduler').setLevel(logging.WARNING)
    
    return logging.getLogger(__name__)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –ª–æ–≥–≥–µ—Ä–∞
logger = setup_logging()

# --- –ö–õ–ê–°–° –î–õ–Ø –ö–≠–®–ò–†–û–í–ê–ù–ò–Ø ---
class GitHubCache:
    """–ö—ç—à –¥–ª—è GitHub API –∑–∞–ø—Ä–æ—Å–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤"""
    
    def __init__(self, cache_file: str, max_age_hours: int = 2):
        self.cache_file = cache_file
        self.max_age_seconds = max_age_hours * 3600
        self.cache = {}
        self.load_cache()
    
    def load_cache(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∫—ç—à –∏–∑ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(self.cache_file):
                with open(self.cache_file, 'r', encoding='utf-8') as f:
                    self.cache = json.load(f)
                logger.info(f"–ö—ç—à –∑–∞–≥—Ä—É–∂–µ–Ω: {len(self.cache)} –∑–∞–ø–∏—Å–µ–π")
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫—ç—à–∞: {e}")
            self.cache = {}
    
    def save_cache(self):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∫—ç—à –≤ —Ñ–∞–π–ª"""
        try:
            with open(self.cache_file, 'w', encoding='utf-8') as f:
                json.dump(self.cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫—ç—à–∞: {e}")
    
    def get(self, key: str) -> Optional[Dict]:
        """–ü–æ–ª—É—á–∞–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞ –µ—Å–ª–∏ –æ–Ω–æ –Ω–µ —É—Å—Ç–∞—Ä–µ–ª–æ"""
        if key in self.cache:
            cache_entry = self.cache[key]
            if time.time() - cache_entry['timestamp'] < self.max_age_seconds:
                return cache_entry['data']
            else:
                # –£–¥–∞–ª—è–µ–º —É—Å—Ç–∞—Ä–µ–≤—à—É—é –∑–∞–ø–∏—Å—å
                del self.cache[key]
        return None
    
    def set(self, key: str, data: Dict):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–Ω–∞—á–µ–Ω–∏–µ –≤ –∫—ç—à"""
        self.cache[key] = {
            'data': data,
            'timestamp': time.time()
        }
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º —Ä–∞–∑–º–µ—Ä –∫—ç—à–∞
        if len(self.cache) > 100:
            # –£–¥–∞–ª—è–µ–º —Å–∞–º—ã–µ —Å—Ç–∞—Ä—ã–µ –∑–∞–ø–∏—Å–∏
            oldest_keys = sorted(self.cache.keys(), 
                               key=lambda k: self.cache[k]['timestamp'])[:20]
            for old_key in oldest_keys:
                del self.cache[old_key]
    
    def cleanup(self):
        """–û—á–∏—â–∞–µ—Ç —É—Å—Ç–∞—Ä–µ–≤—à–∏–µ –∑–∞–ø–∏—Å–∏"""
        current_time = time.time()
        expired_keys = [
            key for key, entry in self.cache.items()
            if current_time - entry['timestamp'] > self.max_age_seconds
        ]
        for key in expired_keys:
            del self.cache[key]
        if expired_keys:
            logger.info(f"–û—á–∏—â–µ–Ω–æ {len(expired_keys)} —É—Å—Ç–∞—Ä–µ–≤—à–∏—Ö –∑–∞–ø–∏—Å–µ–π –∫—ç—à–∞")

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∫—ç—à–∞
github_cache = GitHubCache(CACHE_FILE)

# --- –ö–õ–ê–°–° –î–õ–Ø –£–ü–†–ê–í–õ–ï–ù–ò–Ø –†–ï–°–£–†–°–ê–ú–ò ---
class ResourceManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç —Ä–µ—Å—É—Ä—Å–∞–º–∏ VPS —Å–µ—Ä–≤–µ—Ä–∞"""
    
    def __init__(self):
        self.request_semaphore = asyncio.Semaphore(MAX_CONCURRENT_REQUESTS)
        self.last_memory_check = time.time()
        self.memory_check_interval = 300  # 5 –º–∏–Ω—É—Ç
    
    async def check_memory_usage(self):
        """–ü—Ä–æ–≤–µ—Ä—è–µ—Ç –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏ –∏ –æ—á–∏—â–∞–µ—Ç –µ—Å–ª–∏ –Ω—É–∂–Ω–æ"""
        current_time = time.time()
        if current_time - self.last_memory_check < self.memory_check_interval:
            return
        
        try:
            import psutil
            memory = psutil.virtual_memory()
            if memory.percent > 80:  # –ï—Å–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –±–æ–ª—å—à–µ 80% –ø–∞–º—è—Ç–∏
                logger.warning(f"–í—ã—Å–æ–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏: {memory.percent}%")
                
                # –ü—Ä–∏–Ω—É–¥–∏—Ç–µ–ª—å–Ω–∞—è –æ—á–∏—Å—Ç–∫–∞
                gc.collect()
                github_cache.cleanup()
                
                # –û—á–∏—â–∞–µ–º –∫—ç—à Python
                if hasattr(gc, 'garbage'):
                    gc.garbage.clear()
                
                logger.info("–í—ã–ø–æ–ª–Ω–µ–Ω–∞ –æ—á–∏—Å—Ç–∫–∞ –ø–∞–º—è—Ç–∏")
                
        except ImportError:
            pass  # psutil –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω
        
        self.last_memory_check = current_time
    
    async def get_session(self) -> ClientSession:
        """–°–æ–∑–¥–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é HTTP —Å–µ—Å—Å–∏—é"""
        connector = TCPConnector(
            limit=MAX_CONCURRENT_REQUESTS,
            limit_per_host=2,
            ttl_dns_cache=300,
            use_dns_cache=True
        )
        
        timeout = ClientTimeout(total=REQUEST_TIMEOUT)
        
        return ClientSession(
            connector=connector,
            timeout=timeout,
            headers={'User-Agent': 'GitHub-Release-Monitor-Bot-Optimized'}
        )

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ —Ä–µ—Å—É—Ä—Å–æ–≤
resource_manager = ResourceManager()

# --- –ò–ù–¢–ï–ì–†–ò–†–û–í–ê–ù–ù–ê–Ø –°–ò–°–¢–ï–ú–ê –ü–†–ò–û–†–ò–¢–ï–¢–û–í ---
class RepositoryPriorityManager:
    """–£–ø—Ä–∞–≤–ª—è–µ—Ç –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–µ–π –¥–ª—è VPS"""
    
    def __init__(self):
        self.priority_file = PRIORITY_FILE
        self.priorities = self._load_priorities()
        self.last_priority_update = self._load_last_priority_update()

    def _load_priorities(self) -> Dict[str, Dict]:
        if os.path.exists(self.priority_file):
            try:
                with open(self.priority_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)

                    if isinstance(data, dict) and 'priorities' in data:
                        priorities = data['priorities']
                    else:
                        priorities = data

                    # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –≤—Å–µ—Ö —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤
                    for repo in REPOS:
                        if repo not in priorities:
                            priorities[repo] = self._create_default_priority()
                        else:
                            # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏–µ –ø–æ–ª—è
                            default_priority = self._create_default_priority()
                            for field, default_value in default_priority.items():
                                if field not in priorities[repo]:
                                    priorities[repo][field] = default_value

                    return priorities
            except (json.JSONDecodeError, IOError) as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {e}")
                self._backup_corrupted_file(self.priority_file)

        return {repo: self._create_default_priority() for repo in REPOS}

    def _create_default_priority(self) -> Dict:
        return {
            'update_count': 0,
            'last_update': None,
            'check_interval': DEFAULT_CHECK_INTERVAL_MINUTES,
            'priority_score': 0.0,
            'last_check': None,
            'consecutive_failures': 0,
            'total_checks': 0,
            'average_response_time': 0.0
        }

    def _load_last_priority_update(self) -> Optional[datetime]:
        if os.path.exists(self.priority_file):
            try:
                with open(self.priority_file, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                    if isinstance(data, dict) and 'last_update' in data:
                        return datetime.fromisoformat(data['last_update'])
            except (json.JSONDecodeError, IOError, ValueError):
                pass
        return None

    def _backup_corrupted_file(self, file_path: str):
        """–°–æ–∑–¥–∞–µ—Ç —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"""
        try:
            if os.path.exists(file_path):
                backup_name = f"{os.path.basename(file_path)}.corrupted.{datetime.now().strftime('%Y%m%d_%H%M%S')}"
                backup_path = os.path.join(BACKUP_DIR, backup_name)
                shutil.copy2(file_path, backup_path)
                logger.warning(f"–°–æ–∑–¥–∞–Ω–∞ —Ä–µ–∑–µ—Ä–≤–Ω–∞—è –∫–æ–ø–∏—è –ø–æ–≤—Ä–µ–∂–¥–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞: {backup_path}")
        except Exception as e:
            logger.error(f"–ù–µ —É–¥–∞–ª–æ—Å—å —Å–æ–∑–¥–∞—Ç—å —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é: {e}")

    def _save_priorities(self):
        try:
            # –°–æ–∑–¥–∞–µ–º —Ä–µ–∑–µ—Ä–≤–Ω—É—é –∫–æ–ø–∏—é –ø–µ—Ä–µ–¥ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º
            if os.path.exists(self.priority_file):
                backup_file = f"{self.priority_file}.bak"
                shutil.copy2(self.priority_file, backup_file)

            data = {
                'priorities': self.priorities,
                'last_update': datetime.now(timezone.utc).isoformat(),
                'version': '2.0',
                'repos_count': len(REPOS),
                'created_at': datetime.now(timezone.utc).isoformat(),
                'backup_created': True
            }

            with open(self.priority_file, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)

            logger.debug(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã —É—Å–ø–µ—à–Ω–æ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {self.priority_file}")
        except IOError as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: {e}")

    def get_priority(self, repo: str) -> Dict:
        if repo not in self.priorities:
            self.priorities[repo] = self._create_default_priority()
            self._save_priorities()
        return self.priorities[repo]

    def record_update(self, repo: str):
        priority_data = self.get_priority(repo)
        priority_data['update_count'] += 1
        priority_data['last_update'] = datetime.now(timezone.utc).isoformat()
        priority_data['consecutive_failures'] = 0  # –°–±—Ä–∞—Å—ã–≤–∞–µ–º —Å—á–µ—Ç—á–∏–∫ –æ—à–∏–±–æ–∫
        self._save_priorities()
        logger.info(f"–ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä–æ–≤–∞–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è {repo}. –í—Å–µ–≥–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–π: {priority_data['update_count']}")

    def record_check(self, repo: str, success: bool = True, response_time: float = 0.0):
        """–ó–∞–ø–∏—Å—ã–≤–∞–µ—Ç –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–µ—Ä–∫–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è"""
        priority_data = self.get_priority(repo)
        priority_data['total_checks'] += 1
        priority_data['last_check'] = datetime.now(timezone.utc).isoformat()
        
        if success:
            priority_data['consecutive_failures'] = 0
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ä–µ–¥–Ω–µ–µ –≤—Ä–µ–º—è –æ—Ç–∫–ª–∏–∫–∞
            if priority_data['average_response_time'] > 0:
                priority_data['average_response_time'] = (
                    priority_data['average_response_time'] + response_time
                ) / 2
            else:
                priority_data['average_response_time'] = response_time
        else:
            priority_data['consecutive_failures'] += 1
            
        self._save_priorities()

    def should_update_priorities(self) -> bool:
        if not self.last_priority_update:
            return True
        return datetime.now(timezone.utc) - self.last_priority_update > timedelta(hours=6)

    def update_priorities(self, history_manager=None):
        logger.info("–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤...")

        cutoff_date = datetime.now(timezone.utc) - timedelta(days=PRIORITY_UPDATE_DAYS)
        updated_count = 0

        for repo in REPOS:
            update_count = 0
            # –£–ø—Ä–æ—â–µ–Ω–Ω–∞—è –ª–æ–≥–∏–∫–∞ –¥–ª—è –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ - –∏—Å–ø–æ–ª—å–∑—É–µ–º —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ –¥–∞–Ω–Ω—ã–µ
            if repo in self.priorities:
                update_count = self.priorities[repo].get('update_count', 0)

            priority_score = update_count / PRIORITY_UPDATE_DAYS
            existing_data = self.priorities.get(repo, self._create_default_priority())

            # –£—á–∏—Ç—ã–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –Ω–µ—É–¥–∞—á
            failure_penalty = min(existing_data.get('consecutive_failures', 0) * 0.1, 0.5)
            adjusted_score = max(0, priority_score - failure_penalty)

            # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å —É—á–µ—Ç–æ–º –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –¥–ª—è VPS
            if adjusted_score >= 0.5:  # PRIORITY_THRESHOLD_HIGH
                check_interval = MIN_CHECK_INTERVAL_MINUTES
            elif adjusted_score <= 0.1:  # PRIORITY_THRESHOLD_LOW
                check_interval = MAX_CHECK_INTERVAL_MINUTES
            else:
                ratio = (adjusted_score - 0.1) / (0.5 - 0.1)
                check_interval = int(
                    MAX_CHECK_INTERVAL_MINUTES - ratio * (MAX_CHECK_INTERVAL_MINUTES - MIN_CHECK_INTERVAL_MINUTES)
                )

            new_priority_data = {
                **existing_data,
                'update_count': update_count,
                'check_interval': check_interval,
                'priority_score': round(adjusted_score, 3)
            }

            if (repo not in self.priorities or
                    abs(self.priorities[repo]['check_interval'] - check_interval) > 5 or
                    abs(self.priorities[repo]['priority_score'] - adjusted_score) > 0.01):
                updated_count += 1

            self.priorities[repo] = new_priority_data

        self.last_priority_update = datetime.now(timezone.utc)
        self._save_priorities()

        logger.info(f"–ü—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –æ–±–Ω–æ–≤–ª–µ–Ω—ã. –ò–∑–º–µ–Ω–µ–Ω–æ: {updated_count}/{len(REPOS)} —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤")

        # –î–µ—Ç–∞–ª—å–Ω—ã–π –ª–æ–≥ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
        for repo, data in self.priorities.items():
            status = "üî¥" if data['priority_score'] >= 0.5 else \
                "üü¢" if data['priority_score'] <= 0.1 else "üü°"
            failures = data.get('consecutive_failures', 0)
            failure_info = f" ‚ö†Ô∏è{failures}" if failures > 0 else ""
            logger.info(
                f"{status} {repo}: –∏–Ω—Ç–µ—Ä–≤–∞–ª {data['check_interval']} –º–∏–Ω, "
                f"–ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç {data['priority_score']:.3f}{failure_info}"
            )

    def get_priority_stats(self) -> Dict:
        stats = {
            'high_priority': 0,
            'medium_priority': 0,
            'low_priority': 0,
            'failing_repos': 0,
            'total_repos': len(REPOS),
            'average_interval': 0,
            'total_checks': 0,
            'total_updates': 0
        }

        total_interval = 0
        for repo in REPOS:
            priority_data = self.get_priority(repo)
            score = priority_data['priority_score']
            failures = priority_data.get('consecutive_failures', 0)

            if score >= 0.5:
                stats['high_priority'] += 1
            elif score <= 0.1:
                stats['low_priority'] += 1
            else:
                stats['medium_priority'] += 1

            if failures > 3:
                stats['failing_repos'] += 1

            total_interval += priority_data['check_interval']
            stats['total_checks'] += priority_data.get('total_checks', 0)
            stats['total_updates'] += priority_data.get('update_count', 0)

        stats['average_interval'] = round(total_interval / len(REPOS), 1)
        return stats

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–µ–Ω–µ–¥–∂–µ—Ä–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
priority_manager = RepositoryPriorityManager()

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ó–ê–ì–†–£–ó–ö–ê –ò–ù–§–û–†–ú–ê–¶–ò–ò –û –†–ï–õ–ò–ó–ê–• ---
async def fetch_release_optimized(session: ClientSession, repo_name: str) -> Tuple[Optional[Dict], float]:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –∑–∞–≥—Ä—É–∑–∫–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø–æ—Å–ª–µ–¥–Ω–µ–º —Ä–µ–ª–∏–∑–µ —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º"""
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∫—ç—à
    cache_key = f"release_{repo_name}"
    cached_data = github_cache.get(cache_key)
    if cached_data:
        logger.debug(f"–ò—Å–ø–æ–ª—å–∑—É–µ–º –∫—ç—à–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è {repo_name}")
        return cached_data, 0.0
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –ø–∞–º—è—Ç–∏
    await resource_manager.check_memory_usage()
    
    api_url = f"https://api.github.com/repos/{repo_name}/releases/latest"
    headers = {'User-Agent': 'GitHub-Release-Monitor-Bot-Optimized'}
    
    if GITHUB_TOKEN:
        headers['Authorization'] = f'token {GITHUB_TOKEN}'

    start_time = time.time()
    
    async with resource_manager.request_semaphore:
        for attempt in range(MAX_RETRIES):
            try:
                async with session.get(api_url, headers=headers) as response:
                    response_time = time.time() - start_time
                    
                    if response.status == 200:
                        data = await response.json()
                        
                        # –ö—ç—à–∏—Ä—É–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
                        github_cache.set(cache_key, data)
                        
                        logger.debug(f"–£—Å–ø–µ—à–Ω–æ –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –¥–ª—è {repo_name} –∑–∞ {response_time:.2f}—Å")
                        return data, response_time
                        
                    elif response.status == 403:
                        # Rate limit - —É–≤–µ–ª–∏—á–∏–≤–∞–µ–º –∏–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏
                        logger.warning(f"Rate limit –¥–ª—è {repo_name}")
                        return None, response_time
                        
                    elif response.status == 404:
                        logger.warning(f"–†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –Ω–µ –Ω–∞–π–¥–µ–Ω: {repo_name}")
                        return None, response_time
                        
                    else:
                        logger.warning(f"–°—Ç–∞—Ç—É—Å {response.status} –¥–ª—è {repo_name}")
                        if attempt < MAX_RETRIES - 1:
                            await asyncio.sleep(RETRY_DELAY)
                            continue
                        return None, response_time
                        
            except asyncio.TimeoutError:
                logger.warning(f"Timeout –ø—Ä–∏ –∑–∞–ø—Ä–æ—Å–µ –∫ {repo_name}")
            except Exception as e:
                logger.warning(f"–û—à–∏–±–∫–∞ –∑–∞–ø—Ä–æ—Å–∞ –∫ {repo_name}: {e}")
                
            if attempt < MAX_RETRIES - 1:
                await asyncio.sleep(RETRY_DELAY)
    
    response_time = time.time() - start_time
    return None, response_time

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ü–†–û–í–ï–†–ö–ê –†–ï–ü–û–ó–ò–¢–û–†–ò–ï–í –° –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò ---
async def check_repositories_optimized(bot: Bot):
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
    logger.info("üîÑ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Ä–µ—Å—É—Ä—Å—ã
    await resource_manager.check_memory_usage()
    
    # –û–±–Ω–æ–≤–ª—è–µ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç—ã –µ—Å–ª–∏ –Ω—É–∂–Ω–æ
    if priority_manager.should_update_priorities():
        logger.info("üìä –û–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤...")
        priority_manager.update_priorities()
    
    current_time = datetime.now(timezone.utc)
    repos_to_check = []
    
    # –û–ø—Ä–µ–¥–µ–ª—è–µ–º –∫–∞–∫–∏–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –Ω—É–∂–Ω–æ –ø—Ä–æ–≤–µ—Ä–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤
    for repo_name in REPOS:
        priority_data = priority_manager.get_priority(repo_name)
        check_interval = priority_data['check_interval']
        last_check = priority_data.get('last_check')

        should_check = False
        if not last_check:
            should_check = True
            logger.debug(f"üì¶ {repo_name}: –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
        else:
            try:
                last_check_time = datetime.fromisoformat(last_check)
                time_since_check = current_time - last_check_time
                
                if time_since_check >= timedelta(minutes=check_interval):
                    should_check = True
                    logger.debug(f"üì¶ {repo_name}: –ø—Ä–æ—à–ª–æ {time_since_check}, –∏–Ω—Ç–µ—Ä–≤–∞–ª {check_interval} –º–∏–Ω")
                else:
                    remaining = timedelta(minutes=check_interval) - time_since_check
                    logger.debug(f"üì¶ {repo_name}: –µ—â—ë {remaining} –¥–æ —Å–ª–µ–¥—É—é—â–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –≤—Ä–µ–º–µ–Ω–∏ –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏ –¥–ª—è {repo_name}: {e}")
                should_check = True

        if should_check:
            repos_to_check.append(repo_name)
    
    if not repos_to_check:
        logger.info("–ù–µ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤ –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏")
        return
    
    logger.info(f"üìã –ë—É–¥–µ—Ç –ø—Ä–æ–≤–µ—Ä–µ–Ω–æ {len(repos_to_check)} –∏–∑ {len(REPOS)} —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤")
    
    # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –ø–∞–∫–µ—Ç–∞–º–∏
    for i in range(0, len(repos_to_check), BATCH_SIZE):
        batch = repos_to_check[i:i + BATCH_SIZE]
        
        # –°–æ–∑–¥–∞–µ–º —Å–µ—Å—Å–∏—é –¥–ª—è –ø–∞–∫–µ—Ç–∞
        async with resource_manager.get_session() as session:
            tasks = []
            for repo_name in batch:
                task = check_single_repo_optimized(bot, session, repo_name)
                tasks.append(task)
            
            # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–∞–∫–µ—Ç –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω–æ
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
            for repo_name, result in zip(batch, results):
                if isinstance(result, Exception):
                    logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {repo_name}: {result}")
                elif result:
                    logger.info(f"–ù–∞–π–¥–µ–Ω–æ –æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –¥–ª—è {repo_name}")
                
                # –û–±–Ω–æ–≤–ª—è–µ–º –≤—Ä–µ–º—è –ø–æ—Å–ª–µ–¥–Ω–µ–π –ø—Ä–æ–≤–µ—Ä–∫–∏
                priority_data = priority_manager.get_priority(repo_name)
                priority_data['last_check'] = current_time.isoformat()
                priority_manager._save_priorities()
        
        # –ü–∞—É–∑–∞ –º–µ–∂–¥—É –ø–∞–∫–µ—Ç–∞–º–∏ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏ —Ä–µ—Å—É—Ä—Å–æ–≤
        if i + BATCH_SIZE < len(repos_to_check):
            await asyncio.sleep(5)
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º –∫—ç—à
    github_cache.save_cache()
    
    logger.info("‚úÖ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∑–∞–≤–µ—Ä—à–µ–Ω–∞")

async def check_single_repo_optimized(bot: Bot, session: ClientSession, repo_name: str) -> bool:
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –æ–¥–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è —Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
    try:
        # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø—Ä–æ–≤–µ—Ä–æ–∫
        priority_manager.record_check(repo_name, True, 0.0)
        
        release, response_time = await fetch_release_optimized(session, repo_name)
        
        # –ó–∞–ø–∏—Å—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –ø—Ä–æ–≤–µ—Ä–∫–µ
        success = release is not None
        priority_manager.record_check(repo_name, success, response_time)

        if not release:
            logger.warning(f"‚ùå –ù–µ –ø–æ–ª—É—á–µ–Ω—ã –¥–∞–Ω–Ω—ã–µ –æ —Ä–µ–ª–∏–∑–∞—Ö –¥–ª—è {repo_name}")
            return False

        current_tag = release.get('tag_name')
        if not current_tag:
            logger.warning(f"‚ùå –ù–µ –Ω–∞–π–¥–µ–Ω —Ç–µ–≥ –≤ –¥–∞–Ω–Ω—ã—Ö —Ä–µ–ª–∏–∑–∞ –¥–ª—è {repo_name}")
            return False

        # –ó–¥–µ—Å—å –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –ø—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞ –Ω–æ–≤—ã–µ —Ç–µ–≥–∏
        # –î–ª—è –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º False
        
        return False

    except Exception as e:
        logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø—Ä–æ–≤–µ—Ä–∫–µ {repo_name}: {e}")
        priority_manager.record_check(repo_name, False, 0.0)
        return False

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–´–ô –ü–õ–ê–ù–ò–†–û–í–©–ò–ö ---
def setup_optimized_scheduler(scheduler: AsyncIOScheduler, bot: Bot):
    """–ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ—Ç –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ –¥–ª—è —Å–ª–∞–±–æ–≥–æ VPS —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏"""
    
    # –û—Å–Ω–æ–≤–Ω–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞ –∫–∞–∂–¥—ã–µ 30 –º–∏–Ω—É—Ç (—Å —É—á–µ—Ç–æ–º –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤)
    scheduler.add_job(
        check_repositories_optimized,
        'interval',
        minutes=30,  # –£–≤–µ–ª–∏—á–µ–Ω–æ —Å 15 –¥–æ 30
        kwargs={'bot': bot},
        id='repositories_check_optimized',
        max_instances=1,
        coalesce=True
    )
    
    # –û—á–∏—Å—Ç–∫–∞ –∫—ç—à–∞ –∫–∞–∂–¥—ã–µ 4 —á–∞—Å–∞
    scheduler.add_job(
        lambda: github_cache.cleanup(),
        'interval',
        hours=4,
        id='cache_cleanup',
        max_instances=1
    )
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ—Å—É—Ä—Å–æ–≤ –∫–∞–∂–¥—ã–µ 2 —á–∞—Å–∞
    scheduler.add_job(
        resource_manager.check_memory_usage,
        'interval',
        hours=2,
        id='resource_check',
        max_instances=1
    )
    
    # –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –∫—ç—à–∞ –∫–∞–∂–¥—ã–µ 6 —á–∞—Å–æ–≤
    scheduler.add_job(
        github_cache.save_cache,
        'interval',
        hours=6,
        id='cache_save',
        max_instances=1
    )

# --- –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ì–õ–ê–í–ù–ê–Ø –§–£–ù–ö–¶–ò–Ø ---
async def main_optimized():
    """–û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –¥–ª—è —Å–ª–∞–±–æ–≥–æ VPS —Å —Å–∏—Å—Ç–µ–º–æ–π –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤"""
    print("=" * 50)
    print("üöÄ –ó–ê–ü–£–°–ö –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–û–ì–û –ë–û–¢–ê –° –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò –î–õ–Ø –°–õ–ê–ë–û–ì–û VPS")
    print("=" * 50)

    if not BOT_TOKEN:
        logger.error("‚ùå BOT_TOKEN –Ω–µ –Ω–∞–π–¥–µ–Ω!")
        return

    logger.info("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–æ—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")
    print("ü§ñ –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –±–æ—Ç–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")

    bot = Bot(token=BOT_TOKEN)
    dp = Dispatcher()

    logger.info("‚è∞ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")
    print("‚è∞ –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫–∞ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")
    
    scheduler = AsyncIOScheduler(timezone="UTC")
    setup_optimized_scheduler(scheduler, bot)
    
    scheduler.start()
    logger.info("‚è∞ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∑–∞–ø—É—â–µ–Ω")
    print("‚è∞ –û–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω–∏—Ä–æ–≤—â–∏–∫ —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏ –∑–∞–ø—É—â–µ–Ω")

    print(f"\nüìä –û–ü–¢–ò–ú–ò–ó–ò–†–û–í–ê–ù–ù–ê–Ø –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø –° –ü–†–ò–û–†–ò–¢–ï–¢–ê–ú–ò:")
    print(f"‚îú‚îÄ‚îÄ –†–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–µ–≤: {len(REPOS)}")
    print(f"‚îú‚îÄ‚îÄ –ò–Ω—Ç–µ—Ä–≤–∞–ª –ø—Ä–æ–≤–µ—Ä–∫–∏: {MIN_CHECK_INTERVAL_MINUTES}-{MAX_CHECK_INTERVAL_MINUTES} –º–∏–Ω")
    print(f"‚îú‚îÄ‚îÄ –ú–∞–∫—Å. –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤: {MAX_CONCURRENT_REQUESTS}")
    print(f"‚îú‚îÄ‚îÄ –¢–∞–π–º–∞—É—Ç –∑–∞–ø—Ä–æ—Å–∞: {REQUEST_TIMEOUT} —Å–µ–∫")
    print(f"‚îú‚îÄ‚îÄ –†–∞–∑–º–µ—Ä –ø–∞–∫–µ—Ç–∞: {BATCH_SIZE}")
    print(f"‚îú‚îÄ‚îÄ –ö—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ: –≤–∫–ª—é—á–µ–Ω–æ")
    print(f"‚îî‚îÄ‚îÄ –°–∏—Å—Ç–µ–º–∞ –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–æ–≤: –∞–∫—Ç–∏–≤–Ω–∞")

    try:
        await dp.start_polling(bot, skip_updates=True)
    except KeyboardInterrupt:
        logger.info("‚èπÔ∏è –ü–æ–ª—É—á–µ–Ω —Å–∏–≥–Ω–∞–ª –æ—Å—Ç–∞–Ω–æ–≤–∫–∏")
        print("\n‚èπÔ∏è –û—Å—Ç–∞–Ω–æ–≤–∫–∞...")
    except Exception as e:
        logger.error(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\n‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
    finally:
        logger.info("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        print("üõë –ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ —Ä–∞–±–æ—Ç—ã...")
        
        scheduler.shutdown(wait=True)
        github_cache.save_cache()
        await bot.session.close()
        
        logger.info("‚úÖ –ë–æ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        print("‚úÖ –ë–æ—Ç –ø–æ–ª–Ω–æ—Å—Ç—å—é –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")

# --- –¢–û–ß–ö–ê –í–•–û–î–ê ---
if __name__ == "__main__":
    try:
        logger.info("üöÄ –ó–∞–ø—É—Å–∫ –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è —Å –ø—Ä–∏–æ—Ä–∏—Ç–µ—Ç–∞–º–∏...")
        asyncio.run(main_optimized())
    except KeyboardInterrupt:
        print("\nüëã –ë–æ—Ç –æ—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
    except Exception as e:
        logger.error(f"üí• –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"\nüí• –ö–†–ò–¢–ò–ß–ï–°–ö–ê–Ø –û–®–ò–ë–ö–ê: {e}")
        sys.exit(1)
